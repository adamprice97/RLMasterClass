{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Environments.random_maze import maze_game\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Dict\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.models import MODEL_DEFAULTS\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import copy\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.utils.torch_utils import FLOAT_MIN\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray import air, tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced RL Algorithms and RLLib!** \n",
    "\n",
    "The goal of this second notebook is to show you how I go about solving RL problems, using RLLib and more advanced algorithms. Hopefully it will give you an insight into types of problems that are encountered when working in RL and how they are solved. \n",
    "\n",
    "As a bit of a showcase, we are going to make the maze game for earlier harder, and discuss how to solve it. \n",
    "\n",
    "As in the previous notebook, we will start by looking at problem. You will notice that once again it is a maze game; however, this one will be a fair bit harder to solve. Run the cell below a few times, what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'maze': array([[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "         [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "         [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n",
       "         [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1],\n",
       "         [1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1],\n",
       "         [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]], dtype=int8),\n",
       "  'agent': array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int8),\n",
       "  'goal': array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=int8)},\n",
       " {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = maze_game()\n",
    "example.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the maze game is random! We will need a high level of generalisation to solve this problem! (the action space is the same)\n",
    "\n",
    "You will also notice that the state is split up by a dictionary. The base form (before wrapping) should present as much data as possible, we can always use wrappers later to reduce the data. \n",
    "\n",
    "Like all good ML, we will start with a baseline. We will produce a wrapper that stacks the into 3 planes that will make up the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stacked_maze(maze_game):\n",
    "    #overwrite the init to change the obs space\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #This wrapper updates the observation space. It is now best discribed a multibinary.\n",
    "        self.observation_space = spaces.MultiBinary((12,12,2))\n",
    "    \n",
    "    #overwrite create_state to change how the env handles the state\n",
    "    def create_state(self): #In this version, the goal never changes, so ignore it\n",
    "        state = np.stack([np.array(self.maze==self.wall, dtype=np.int8),\n",
    "                          np.array(self.maze==self.agent, dtype=np.int8)], axis=-1)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can introduce RLLib, we start by registiering the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to registier the env we want to use with RLLib before we can use it:\n",
    "def env_creator(env_config):\n",
    "    return stacked_maze()\n",
    "register_env(\"stacked_maze\", env_creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the model that we want to use. More on this config can be found here: https://github.com/ray-project/ray/blob/469f4d296a112f2ade556ea586a0f05811b34d32/rllib/models/catalog.py#L52\n",
    "\n",
    "We are using some convolution layers to reduce the dimensionality of the matrix inputs. These will get flattened and passed it the dense layers given in 'fcnet_hiddens'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.copy(MODEL_DEFAULTS)\n",
    "model.update({'fcnet_hiddens': [256, 128], 'fcnet_activation': 'relu', 'conv_filters': [[4, [3, 3], 1], [8, [3, 3], 1], [12, [3, 3], 1], [16, [3, 3], 1]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a config that will produce a trainer for learning on the \"stacked_maze\" environment. We wont worry too much about the learning params rn. At this stage I am mainly checking that my MDP is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 11:58:27,912\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 11:58:27,914\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 11:58:27,917\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/adamprice/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/adamprice/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/adamprice/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/adamprice/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-10-25 11:58:27,927\tINFO tensorboardx.py:48 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2023-10-25 11:58:27,929\tWARNING unified.py:56 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.\n",
      "2023-10-25 11:58:31,691\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(pid=3789)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3789)\u001b[0m 2023-10-25 11:58:41,384\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3786)\u001b[0m 2023-10-25 11:58:41,552\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.complex_input_net.ComplexInputNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3786)\u001b[0m 2023-10-25 11:58:41,552\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3786)\u001b[0m 2023-10-25 11:58:41,553\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3786)\u001b[0m 2023-10-25 11:58:41,574\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3786)\u001b[0m 2023-10-25 11:58:41,574\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3786)\u001b[0m 2023-10-25 11:58:41,575\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3786)\u001b[0m 2023-10-25 11:58:42,667\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3786)\u001b[0m 2023-10-25 11:58:42,667\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3786)\u001b[0m 2023-10-25 11:58:42,667\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3786)\u001b[0m 2023-10-25 11:58:42,667\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3786)\u001b[0m 2023-10-25 11:58:42,672\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "2023-10-25 11:58:42,709\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 11:58:42,713\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.complex_input_net.ComplexInputNetwork` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 11:58:42,714\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "2023-10-25 11:58:42,715\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 11:58:42,731\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 11:58:42,732\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 11:58:42,733\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 11:58:43,317\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 11:58:43,318\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 11:58:43,319\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 11:58:43,319\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 11:58:43,327\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "2023-10-25 11:58:43,359\tINFO trainable.py:172 -- Trainable.setup took 15.429 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-10-25 11:58:43,362\tWARNING util.py:68 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .rl_module(_enable_rl_module_api=False)\n",
    "    .environment(env=\"stacked_maze\")\\\n",
    "    .rollouts(num_rollout_workers=4, num_envs_per_worker=1)\\\n",
    "    .training(_enable_learner_api=False, train_batch_size=2000, gamma=0.995, model=model, lr=0.001,  )\\\n",
    "    .environment(disable_env_checking=True)\\\n",
    "    .framework('torch')\\\n",
    ")\n",
    "trainer = config.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the trainer is built, we can start training! We are simply powering this with a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 11:58:44,925\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "   2 \tr_mean: -32.1 \tr_max: 22.9 \tr_min: -50.0\n",
      "   3 \tr_mean: -32.7 \tr_max: 22.9 \tr_min: -50.0\n",
      "   4 \tr_mean: -37.0 \tr_max: 22.9 \tr_min: -50.0\n",
      "   5 \tr_mean: -39.6 \tr_max: 22.9 \tr_min: -50.0\n",
      "   6 \tr_mean: -41.4 \tr_max: 22.9 \tr_min: -50.0\n",
      "   7 \tr_mean: -42.6 \tr_max: 22.9 \tr_min: -50.0\n",
      "   8 \tr_mean: -43.5 \tr_max: 22.9 \tr_min: -50.0\n",
      "   9 \tr_mean: -44.2 \tr_max: 22.9 \tr_min: -50.0\n",
      "  10 \tr_mean: -44.8 \tr_max: 22.9 \tr_min: -50.0\n",
      "  11 \tr_mean: -45.3 \tr_max: 22.9 \tr_min: -50.0\n",
      "  12 \tr_mean: -45.7 \tr_max: 22.9 \tr_min: -50.0\n",
      "  13 \tr_mean: -46.0 \tr_max: 22.9 \tr_min: -50.0\n",
      "  14 \tr_mean: -46.3 \tr_max: 22.9 \tr_min: -50.0\n",
      "  15 \tr_mean: -46.5 \tr_max: 22.9 \tr_min: -50.0\n",
      "  16 \tr_mean: -46.8 \tr_max: 22.9 \tr_min: -50.0\n",
      "  17 \tr_mean: -46.9 \tr_max: 22.9 \tr_min: -50.0\n",
      "  18 \tr_mean: -47.1 \tr_max: 22.9 \tr_min: -50.0\n",
      "  19 \tr_mean: -47.3 \tr_max: 22.9 \tr_min: -50.0\n",
      "  20 \tr_mean: -47.4 \tr_max: 22.9 \tr_min: -50.0\n",
      "  21 \tr_mean: -47.5 \tr_max: 22.9 \tr_min: -50.0\n",
      "  22 \tr_mean: -47.6 \tr_max: 22.9 \tr_min: -50.0\n",
      "  23 \tr_mean: -47.7 \tr_max: 22.9 \tr_min: -50.0\n",
      "  24 \tr_mean: -47.8 \tr_max: 22.9 \tr_min: -50.0\n",
      "  25 \tr_mean: -47.9 \tr_max: 22.9 \tr_min: -50.0\n",
      "  26 \tr_mean: -47.9 \tr_max: 22.9 \tr_min: -50.0\n",
      "  27 \tr_mean: -49.4 \tr_max: 13.9 \tr_min: -50.0\n",
      "  28 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  29 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  30 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  31 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  32 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  33 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  34 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  35 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  36 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  37 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  38 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  39 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  40 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  41 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  42 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  43 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  44 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  45 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  46 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  47 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  48 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  49 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n",
      "  50 \tr_mean: -50.0 \tr_max: -50.0 \tr_min: -50.0\n"
     ]
    }
   ],
   "source": [
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "\n",
    "for i in range(50):\n",
    "    print_results(trainer.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this didn't get a very good result, but it seems to have learnt something! So its now time to consider what hyperparamters we are using. In RL, typically have 5-10 different hyperparameters to balance, which makes running hyperparameter searches very time consuming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 12:03:36,445\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:03:36,447\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:03:36,450\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/adamprice/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/adamprice/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/adamprice/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/adamprice/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "\u001b[2m\u001b[36m(pid=5922)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5922)\u001b[0m 2023-10-25 12:03:46,903\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5920)\u001b[0m 2023-10-25 12:03:47,082\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.complex_input_net.ComplexInputNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5920)\u001b[0m 2023-10-25 12:03:47,096\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5920)\u001b[0m 2023-10-25 12:03:47,097\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5920)\u001b[0m 2023-10-25 12:03:47,108\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5920)\u001b[0m 2023-10-25 12:03:47,108\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5920)\u001b[0m 2023-10-25 12:03:47,108\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 12:03:47,739\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:03:47,779\tINFO trainable.py:172 -- Trainable.setup took 11.322 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-10-25 12:03:47,782\tWARNING util.py:68 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .rl_module(_enable_rl_module_api=False)\n",
    "    .environment(env=\"stacked_maze\")\\\n",
    "    .rollouts(num_rollout_workers=4, num_envs_per_worker=1)\\\n",
    "    .training(_enable_learner_api=False, train_batch_size=15000, gamma=0.995, model=model, lr=0.0003,\\\n",
    "              clip_param=0.15, num_sgd_iter=4, sgd_minibatch_size=3000, entropy_coeff=0.0001,)\\\n",
    "    .environment(disable_env_checking=True)\\\n",
    "    .framework('torch')\\\n",
    ")\n",
    "trainer = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=5920)\u001b[0m 2023-10-25 12:03:47,699\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5920)\u001b[0m 2023-10-25 12:03:47,699\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5920)\u001b[0m 2023-10-25 12:03:47,699\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5920)\u001b[0m 2023-10-25 12:03:47,699\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5920)\u001b[0m 2023-10-25 12:03:47,704\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 \tr_mean: -45.9 \tr_max: 12.9 \tr_min: -50.0\n",
      "   2 \tr_mean: -33.6 \tr_max: 40.5 \tr_min: -50.0\n",
      "   3 \tr_mean: -24.3 \tr_max: 40.5 \tr_min: -50.0\n",
      "   4 \tr_mean: -16.7 \tr_max: 42.9 \tr_min: -50.0\n",
      "   5 \tr_mean: -18.7 \tr_max: 42.9 \tr_min: -50.0\n",
      "   6 \tr_mean: -21.3 \tr_max: 42.9 \tr_min: -50.0\n",
      "   7 \tr_mean: -21.0 \tr_max: 39.4 \tr_min: -50.0\n",
      "   8 \tr_mean: -21.0 \tr_max: 43.9 \tr_min: -50.0\n",
      "   9 \tr_mean: -14.5 \tr_max: 43.9 \tr_min: -50.0\n",
      "  10 \tr_mean: -13.3 \tr_max: 43.9 \tr_min: -50.0\n",
      "  11 \tr_mean: -7.4 \tr_max: 42.8 \tr_min: -50.0\n",
      "  12 \tr_mean: -3.6 \tr_max: 42.8 \tr_min: -50.0\n",
      "  13 \tr_mean: -5.4 \tr_max: 41.2 \tr_min: -50.0\n",
      "  14 \tr_mean: -14.9 \tr_max: 39.8 \tr_min: -50.0\n",
      "  15 \tr_mean: -10.5 \tr_max: 44.0 \tr_min: -50.0\n",
      "  16 \tr_mean: -6.9 \tr_max: 44.0 \tr_min: -50.0\n",
      "  17 \tr_mean: -3.9 \tr_max: 44.0 \tr_min: -50.0\n",
      "  18 \tr_mean: -6.1 \tr_max: 43.7 \tr_min: -50.0\n",
      "  19 \tr_mean: -4.6 \tr_max: 45.0 \tr_min: -50.0\n",
      "  20 \tr_mean: -8.0 \tr_max: 45.0 \tr_min: -50.0\n",
      "  21 \tr_mean: -9.8 \tr_max: 44.7 \tr_min: -50.0\n",
      "  22 \tr_mean: -8.2 \tr_max: 45.2 \tr_min: -50.0\n",
      "  23 \tr_mean: -6.1 \tr_max: 45.9 \tr_min: -50.0\n",
      "  24 \tr_mean: -8.9 \tr_max: 45.9 \tr_min: -50.0\n",
      "  25 \tr_mean: -4.6 \tr_max: 44.8 \tr_min: -50.0\n",
      "  26 \tr_mean: -6.1 \tr_max: 46.2 \tr_min: -50.0\n",
      "  27 \tr_mean: -10.8 \tr_max: 46.2 \tr_min: -50.0\n",
      "  28 \tr_mean: -12.4 \tr_max: 46.2 \tr_min: -50.0\n",
      "  29 \tr_mean: -14.5 \tr_max: 45.6 \tr_min: -50.0\n",
      "  30 \tr_mean: -12.7 \tr_max: 44.8 \tr_min: -50.0\n",
      "  31 \tr_mean: -13.8 \tr_max: 44.4 \tr_min: -50.0\n",
      "  32 \tr_mean: -12.7 \tr_max: 44.4 \tr_min: -50.0\n",
      "  33 \tr_mean: -10.3 \tr_max: 44.4 \tr_min: -50.0\n",
      "  34 \tr_mean: -4.4 \tr_max: 44.2 \tr_min: -50.0\n",
      "  35 \tr_mean: -5.3 \tr_max: 44.8 \tr_min: -50.0\n",
      "  36 \tr_mean: -11.4 \tr_max: 44.8 \tr_min: -50.0\n",
      "  37 \tr_mean: -12.9 \tr_max: 44.8 \tr_min: -50.0\n",
      "  38 \tr_mean: -4.6 \tr_max: 46.3 \tr_min: -50.0\n",
      "  39 \tr_mean: -6.8 \tr_max: 46.3 \tr_min: -50.0\n",
      "  40 \tr_mean: -13.5 \tr_max: 46.0 \tr_min: -50.0\n",
      "  41 \tr_mean: -17.1 \tr_max: 45.4 \tr_min: -50.0\n",
      "  42 \tr_mean: -5.4 \tr_max: 46.1 \tr_min: -50.0\n",
      "  43 \tr_mean: -5.8 \tr_max: 46.1 \tr_min: -50.0\n",
      "  44 \tr_mean: -12.2 \tr_max: 45.5 \tr_min: -50.0\n",
      "  45 \tr_mean: -14.5 \tr_max: 45.5 \tr_min: -50.0\n",
      "  46 \tr_mean: -3.4 \tr_max: 44.9 \tr_min: -50.0\n",
      "  47 \tr_mean: -5.9 \tr_max: 44.9 \tr_min: -50.0\n",
      "  48 \tr_mean: -6.4 \tr_max: 45.5 \tr_min: -50.0\n",
      "  49 \tr_mean: -5.5 \tr_max: 45.5 \tr_min: -50.0\n",
      "  50 \tr_mean: -10.4 \tr_max: 44.8 \tr_min: -50.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print_results(trainer.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_maze_action_mask(maze_game):\n",
    "    #overwrite the init to change the obs space\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #This wrapper updates the observation space. It is now best discribed a multibinary.\n",
    "        self.observation_space = spaces.Dict({'observations': spaces.MultiBinary((12,12,2)), \n",
    "                                              'action_mask': spaces.MultiBinary(4)})\n",
    "    \n",
    "    #overwrite create_state to change how the env handles the state\n",
    "    def create_state(self):\n",
    "        state = {}\n",
    "        state['observations'] = np.stack([np.array(self.maze==self.wall, dtype=np.int8),\n",
    "                                  np.array(self.maze==self.agent, dtype=np.int8)], axis=-1)\n",
    "        state['action_mask'] = self.get_action_mask()\n",
    "        return state\n",
    "    \n",
    "    def get_action_mask(self):\n",
    "        action_mask = np.zeros(4,dtype=np.int8)\n",
    "        if self.y != 0:\n",
    "            if self.maze[self.x, self.y-1] != self.wall:\n",
    "                action_mask[0] = 1\n",
    "        if self.x != self.maze.shape[0]-1:\n",
    "            if self.maze[self.x+1, self.y] != self.wall:\n",
    "                action_mask[1] = 1\n",
    "        if self.y != self.maze.shape[1]-1:\n",
    "            if self.maze[self.x, self.y+1] != self.wall:\n",
    "                action_mask[2] = 1\n",
    "        if self.x != 0:\n",
    "            if self.maze[self.x-1, self.y] != self.wall:\n",
    "                action_mask[3] = 1\n",
    "        return action_mask\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return random_maze_action_mask()\n",
    "register_env(\"random_maze_action_mask\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch, nn = try_import_torch()\n",
    "\n",
    "class ActionMaskModel(TorchModelV2, nn.Module):\n",
    "    \"\"\"PyTorch version of above ActionMaskingModel.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        model_config,\n",
    "        name,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        orig_space = getattr(obs_space, \"original_space\", obs_space)\n",
    "        assert (\n",
    "            isinstance(orig_space, Dict)\n",
    "            and \"action_mask\" in orig_space.spaces\n",
    "            and \"observations\" in orig_space.spaces\n",
    "        )\n",
    "\n",
    "        TorchModelV2.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name, **kwargs\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.internal_model = TorchFC(\n",
    "            orig_space[\"observations\"],\n",
    "            action_space,\n",
    "            num_outputs,\n",
    "            model_config,\n",
    "            name + \"_internal\",\n",
    "        )\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Extract the available actions tensor from the observation.\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "        # Compute the unmasked logits.\n",
    "        logits, _ = self.internal_model({\"obs\": input_dict[\"obs\"][\"observations\"]})\n",
    "        # Convert action_mask into a [0.0 || -inf]-type mask.\n",
    "        inf_mask = torch.clamp(torch.log(action_mask), min=FLOAT_MIN)\n",
    "        masked_logits = logits + inf_mask\n",
    "        # Return masked logits.\n",
    "        return masked_logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.internal_model.value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 12:10:35,182\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:10:35,184\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:10:35,187\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=8432)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8432)\u001b[0m 2023-10-25 12:10:41,788\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8432)\u001b[0m /var/folders/3g/81gb31hd25n_14vp4b357xgc0000gn/T/ipykernel_3194/682963800.py:27: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m 2023-10-25 12:10:41,792\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m 2023-10-25 12:10:41,792\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m 2023-10-25 12:10:41,799\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m 2023-10-25 12:10:41,800\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m 2023-10-25 12:10:41,800\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m 2023-10-25 12:10:42,255\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m 2023-10-25 12:10:42,256\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m 2023-10-25 12:10:42,256\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m 2023-10-25 12:10:42,256\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m 2023-10-25 12:10:42,259\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "2023-10-25 12:10:42,285\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/var/folders/3g/81gb31hd25n_14vp4b357xgc0000gn/T/ipykernel_3194/682963800.py:27: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  self.internal_model = TorchFC(\n",
      "2023-10-25 12:10:42,304\tWARNING util.py:68 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "ModelCatalog.register_custom_model(\"RandomMazeModel\", ActionMaskModel)\n",
    "model = copy.copy(MODEL_DEFAULTS)\n",
    "model.update({'custom_model': \"RandomMazeModel\", 'custom_model_config': {},})\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .rl_module(_enable_rl_module_api=False)\n",
    "    .environment(env=\"random_maze_action_mask\")\\\n",
    "    .rollouts(num_rollout_workers=4, num_envs_per_worker=1)\\\n",
    "    .training(_enable_learner_api=False, train_batch_size=15000, gamma=0.995, model=model, lr=0.0003,\\\n",
    "              clip_param=0.15, num_sgd_iter=4, sgd_minibatch_size=3000, entropy_coeff=0.0001,)\\\n",
    "    .environment(disable_env_checking=True)\\\n",
    "    .framework('torch')\\\n",
    ")\n",
    "trainer = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m /Users/adamprice/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8430)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 \tr_mean: -17.6 \tr_max: 34.9 \tr_min: -50.0\n",
      "   2 \tr_mean: -15.2 \tr_max: 44.9 \tr_min: -50.0\n",
      "   3 \tr_mean: -6.6 \tr_max: 45.3 \tr_min: -50.0\n",
      "   4 \tr_mean: 14.8 \tr_max: 47.1 \tr_min: -50.0\n",
      "   5 \tr_mean: 10.6 \tr_max: 47.1 \tr_min: -50.0\n",
      "   6 \tr_mean: 0.4 \tr_max: 46.3 \tr_min: -50.0\n",
      "   7 \tr_mean: 6.2 \tr_max: 46.9 \tr_min: -50.0\n",
      "   8 \tr_mean: 11.4 \tr_max: 46.7 \tr_min: -50.0\n",
      "   9 \tr_mean: 15.5 \tr_max: 46.7 \tr_min: -50.0\n",
      "  10 \tr_mean: 8.9 \tr_max: 46.1 \tr_min: -50.0\n",
      "  11 \tr_mean: 5.4 \tr_max: 46.3 \tr_min: -50.0\n",
      "  12 \tr_mean: 1.5 \tr_max: 46.9 \tr_min: -50.0\n",
      "  13 \tr_mean: 6.4 \tr_max: 46.9 \tr_min: -50.0\n",
      "  14 \tr_mean: 11.4 \tr_max: 47.5 \tr_min: -50.0\n",
      "  15 \tr_mean: 5.2 \tr_max: 47.5 \tr_min: -50.0\n",
      "  16 \tr_mean: 4.9 \tr_max: 47.3 \tr_min: -50.0\n",
      "  17 \tr_mean: 6.6 \tr_max: 46.7 \tr_min: -50.0\n",
      "  18 \tr_mean: 2.1 \tr_max: 47.3 \tr_min: -50.0\n",
      "  19 \tr_mean: -0.8 \tr_max: 47.7 \tr_min: -50.0\n",
      "  20 \tr_mean: 1.9 \tr_max: 47.7 \tr_min: -50.0\n",
      "  21 \tr_mean: 8.8 \tr_max: 47.5 \tr_min: -50.0\n",
      "  22 \tr_mean: 5.8 \tr_max: 47.5 \tr_min: -50.0\n",
      "  23 \tr_mean: 6.8 \tr_max: 47.5 \tr_min: -50.0\n",
      "  24 \tr_mean: 15.3 \tr_max: 47.5 \tr_min: -50.0\n",
      "  25 \tr_mean: 19.4 \tr_max: 47.5 \tr_min: -50.0\n",
      "  26 \tr_mean: 12.2 \tr_max: 47.1 \tr_min: -50.0\n",
      "  27 \tr_mean: 9.0 \tr_max: 46.5 \tr_min: -50.0\n",
      "  28 \tr_mean: 11.9 \tr_max: 46.9 \tr_min: -50.0\n",
      "  29 \tr_mean: 2.1 \tr_max: 47.1 \tr_min: -50.0\n",
      "  30 \tr_mean: 2.4 \tr_max: 47.3 \tr_min: -50.0\n",
      "  31 \tr_mean: 6.1 \tr_max: 47.3 \tr_min: -50.0\n",
      "  32 \tr_mean: 11.5 \tr_max: 47.1 \tr_min: -50.0\n",
      "  33 \tr_mean: 7.3 \tr_max: 47.3 \tr_min: -50.0\n",
      "  34 \tr_mean: 3.5 \tr_max: 47.7 \tr_min: -50.0\n",
      "  35 \tr_mean: -0.3 \tr_max: 47.7 \tr_min: -50.0\n",
      "  36 \tr_mean: 5.1 \tr_max: 47.3 \tr_min: -50.0\n",
      "  37 \tr_mean: 9.3 \tr_max: 47.1 \tr_min: -50.0\n",
      "  38 \tr_mean: 5.9 \tr_max: 46.5 \tr_min: -50.0\n",
      "  39 \tr_mean: 12.5 \tr_max: 46.5 \tr_min: -50.0\n",
      "  40 \tr_mean: 13.1 \tr_max: 47.5 \tr_min: -50.0\n",
      "  41 \tr_mean: 19.5 \tr_max: 47.5 \tr_min: -50.0\n",
      "  42 \tr_mean: 9.1 \tr_max: 47.5 \tr_min: -50.0\n",
      "  43 \tr_mean: 14.6 \tr_max: 47.9 \tr_min: -50.0\n",
      "  44 \tr_mean: 11.1 \tr_max: 47.9 \tr_min: -50.0\n",
      "  45 \tr_mean: -3.0 \tr_max: 46.9 \tr_min: -50.0\n",
      "  46 \tr_mean: 2.6 \tr_max: 47.3 \tr_min: -50.0\n",
      "  47 \tr_mean: 5.0 \tr_max: 47.3 \tr_min: -50.0\n",
      "  48 \tr_mean: -0.9 \tr_max: 47.3 \tr_min: -50.0\n",
      "  49 \tr_mean: 4.4 \tr_max: 47.9 \tr_min: -50.0\n",
      "  50 \tr_mean: 9.3 \tr_max: 47.5 \tr_min: -50.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print_results(trainer.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-10-25 12:51:07</td></tr>\n",
       "<tr><td>Running for: </td><td>00:23:48.95        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.5/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 5.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                             </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  num_sgd_iter</th><th style=\"text-align: right;\">  sgd_minibatch_size</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00001</td><td>RUNNING   </td><td>127.0.0.1:19055</td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         661.39 </td><td style=\"text-align: right;\">1212000</td><td style=\"text-align: right;\">  -4.105</td><td style=\"text-align: right;\">                47.9</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            296.56</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00002</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             7</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00003</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00004</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00005</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             7</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00006</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00007</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00008</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             7</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00009</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00010</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00011</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             7</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00012</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00013</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00014</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             7</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00015</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00016</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00017</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">             7</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_74bab_00000</td><td>TERMINATED</td><td>127.0.0.1:14587</td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         730.925</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">  -7.638</td><td style=\"text-align: right;\">                46.9</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            316.86</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 12:27:18,468\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,469\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,474\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,475\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,479\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,480\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,491\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,493\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,500\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,501\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,508\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,509\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,515\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,518\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,528\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,529\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,536\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,537\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,543\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,545\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,551\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,552\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,558\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,559\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,584\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,585\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,592\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,594\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,600\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,602\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 12:27:18,610\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:18,611\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(pid=14587)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:26,452\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:26,452\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:26,453\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=14634)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=14633)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14634)\u001b[0m 2023-10-25 12:27:33,463\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14634)\u001b[0m /var/folders/3g/81gb31hd25n_14vp4b357xgc0000gn/T/ipykernel_3194/682963800.py:27: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m 2023-10-25 12:27:33,606\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m 2023-10-25 12:27:33,606\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m 2023-10-25 12:27:33,613\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m 2023-10-25 12:27:33,613\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m 2023-10-25 12:27:33,613\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m 2023-10-25 12:27:34,069\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m 2023-10-25 12:27:34,069\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m 2023-10-25 12:27:34,069\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m 2023-10-25 12:27:34,069\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m 2023-10-25 12:27:34,072\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "2023-10-25 12:27:34,474\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:27:34,475\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m /Users/adamprice/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14630)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(pid=14630)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:34,097\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m /var/folders/3g/81gb31hd25n_14vp4b357xgc0000gn/T/ipykernel_3194/682963800.py:27: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:34,099\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:34,099\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:34,104\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:34,104\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:34,104\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:34,446\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:34,446\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:34,446\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:34,446\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:34,449\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14587)\u001b[0m 2023-10-25 12:27:39,889\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=19055)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:39:54,284\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:39:54,284\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:39:54,284\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=19105)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m 2023-10-25 12:40:00,393\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m 2023-10-25 12:40:00,532\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m 2023-10-25 12:40:00,532\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m /var/folders/3g/81gb31hd25n_14vp4b357xgc0000gn/T/ipykernel_3194/682963800.py:27: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m 2023-10-25 12:40:00,541\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m 2023-10-25 12:40:00,541\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m 2023-10-25 12:40:00,541\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m 2023-10-25 12:40:00,967\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m 2023-10-25 12:40:00,967\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m 2023-10-25 12:40:00,968\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m 2023-10-25 12:40:00,968\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m 2023-10-25 12:40:00,970\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "2023-10-25 12:40:01,371\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 12:40:01,372\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:06,757\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=19107)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:00,994\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:00,997\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:00,997\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m /var/folders/3g/81gb31hd25n_14vp4b357xgc0000gn/T/ipykernel_3194/682963800.py:27: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:01,002\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:01,002\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:01,002\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:01,343\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:01,343\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:01,343\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:01,343\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19055)\u001b[0m 2023-10-25 12:40:01,346\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m /Users/adamprice/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19105)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "2023-10-25 12:51:07,375\tWARNING tune.py:192 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-10-25 12:51:10,513\tINFO tune.py:1148 -- Total run time: 1432.09 seconds (1428.93 seconds for the tuning loop).\n",
      "2023-10-25 12:51:10,514\tWARNING tune.py:1163 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/Users/adamprice/RL Master Class/RLMasterClass/results/ppo_hyperparam_search\", trainable=...)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Uploading, downloading, and deleting from cloud storage requires pyarrow to be installed. Install with: `pip install pyarrow`. Subsequent calls to cloud operations will be ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[39m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     PPOConfig()\n\u001b[1;32m      3\u001b[0m     \u001b[39m.\u001b[39mrl_module(_enable_rl_module_api\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39m.\u001b[39mframework(\u001b[39m'\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m'\u001b[39m)\\\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     12\u001b[0m tuner \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39mTuner(\n\u001b[1;32m     13\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     tune_config\u001b[39m=\u001b[39mtune\u001b[39m.\u001b[39mTuneConfig(num_samples\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m),\n\u001b[1;32m     15\u001b[0m     param_space\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mto_dict(),\n\u001b[1;32m     16\u001b[0m     run_config\u001b[39m=\u001b[39mair\u001b[39m.\u001b[39mRunConfig(stop\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mtimesteps_total\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1.5e6\u001b[39m}, storage_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mppo_hyperparam_search\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> 18\u001b[0m results \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/tuner.py:347\u001b[0m, in \u001b[0;36mTuner.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_ray_client:\n\u001b[1;32m    346\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_local_tuner\u001b[39m.\u001b[39;49mfit()\n\u001b[1;32m    348\u001b[0m     \u001b[39mexcept\u001b[39;00m TuneError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    349\u001b[0m         \u001b[39mraise\u001b[39;00m TuneError(\n\u001b[1;32m    350\u001b[0m             _TUNER_FAILED_MSG\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    351\u001b[0m                 path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_tuner\u001b[39m.\u001b[39mget_experiment_checkpoint_dir()\n\u001b[1;32m    352\u001b[0m             )\n\u001b[1;32m    353\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:588\u001b[0m, in \u001b[0;36mTunerInternal.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m param_space \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_space)\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_restored:\n\u001b[0;32m--> 588\u001b[0m     analysis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_internal(trainable, param_space)\n\u001b[1;32m    589\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m     analysis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_resume(trainable, param_space)\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:703\u001b[0m, in \u001b[0;36mTunerInternal._fit_internal\u001b[0;34m(self, trainable, param_space)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fitting for a fresh Tuner.\"\"\"\u001b[39;00m\n\u001b[1;32m    690\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[1;32m    691\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tune_run_arguments(trainable),\n\u001b[1;32m    692\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tuner_kwargs,\n\u001b[1;32m    702\u001b[0m }\n\u001b[0;32m--> 703\u001b[0m analysis \u001b[39m=\u001b[39m run(\n\u001b[1;32m    704\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs,\n\u001b[1;32m    705\u001b[0m )\n\u001b[1;32m    706\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclear_remote_string_queue()\n\u001b[1;32m    707\u001b[0m \u001b[39mreturn\u001b[39;00m analysis\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/tune.py:1167\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1163\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1164\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExperiment has been interrupted, but the most recent state was \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1165\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msaved.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mResume experiment with: \u001b[39m\u001b[39m{\u001b[39;00mrestore_entrypoint\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1166\u001b[0m         )\n\u001b[0;32m-> 1167\u001b[0m ea \u001b[39m=\u001b[39m ExperimentAnalysis(\n\u001b[1;32m   1168\u001b[0m     experiment_checkpoint,\n\u001b[1;32m   1169\u001b[0m     trials\u001b[39m=\u001b[39;49mall_trials,\n\u001b[1;32m   1170\u001b[0m     default_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1171\u001b[0m     default_mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   1172\u001b[0m     remote_storage_path\u001b[39m=\u001b[39;49mremote_path,\n\u001b[1;32m   1173\u001b[0m )\n\u001b[1;32m   1175\u001b[0m \u001b[39mreturn\u001b[39;00m ea\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/analysis/experiment_analysis.py:114\u001b[0m, in \u001b[0;36mExperimentAnalysis.__init__\u001b[0;34m(self, experiment_checkpoint_path, trials, default_metric, default_mode, remote_storage_path, sync_config)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment_states \u001b[39m=\u001b[39m []\n\u001b[1;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoints_and_paths: List[Tuple[\u001b[39mdict\u001b[39m, os\u001b[39m.\u001b[39mPathLike]] \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_checkpoints(experiment_checkpoint_path)\n\u001b[1;32m    115\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoints_and_paths\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials \u001b[39m=\u001b[39m trials\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/analysis/experiment_analysis.py:194\u001b[0m, in \u001b[0;36mExperimentAnalysis._load_checkpoints\u001b[0;34m(self, experiment_checkpoint_path)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_checkpoints\u001b[39m(\u001b[39mself\u001b[39m, experiment_checkpoint_path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m    193\u001b[0m     \u001b[39m# Get the latest checkpoints from the checkpoint_path.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     latest_checkpoints \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_latest_checkpoint(experiment_checkpoint_path)\n\u001b[1;32m    195\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m latest_checkpoints:\n\u001b[1;32m    196\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00mexperiment_checkpoint_path\u001b[39m}\u001b[39;00m\u001b[39m` must either be a path to an \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexperiment checkpoint file, or a directory containing an experiment \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcheckpoint file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m         )\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/tune/analysis/experiment_analysis.py:314\u001b[0m, in \u001b[0;36mExperimentAnalysis._get_latest_checkpoint\u001b[0;34m(self, experiment_checkpoint_path)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_latest_checkpoint\u001b[39m(\u001b[39mself\u001b[39m, experiment_checkpoint_path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m    301\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Gets the latest experiment checkpoints corresponding to a given path.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[1;32m    303\u001b[0m \u001b[39m    Acceptable path inputs (either local or remote):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m        file for each experiment corresponding to the given path.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mif\u001b[39;00m is_directory(experiment_checkpoint_path):\n\u001b[1;32m    315\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_latest_checkpoint_from_dir(experiment_checkpoint_path)\n\u001b[1;32m    317\u001b[0m     local_experiment_checkpoint_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_download_experiment_checkpoint(\n\u001b[1;32m    318\u001b[0m         experiment_checkpoint_path\n\u001b[1;32m    319\u001b[0m     )\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:617\u001b[0m, in \u001b[0;36mis_directory\u001b[0;34m(uri)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_directory\u001b[39m(uri: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    609\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Checks if a remote URI is a directory or a file.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \n\u001b[1;32m    611\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[39m        FileNotFoundError: if the URI doesn't exist.\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m     _assert_pyarrow_installed()\n\u001b[1;32m    619\u001b[0m     fs, bucket_path \u001b[39m=\u001b[39m get_fs_and_path(uri)\n\u001b[1;32m    620\u001b[0m     file_info \u001b[39m=\u001b[39m fs\u001b[39m.\u001b[39mget_file_info(bucket_path)\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/testtrainingenv/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:104\u001b[0m, in \u001b[0;36m_assert_pyarrow_installed\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_pyarrow_installed\u001b[39m():\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m pyarrow \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    105\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUploading, downloading, and deleting from cloud storage \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires pyarrow to be installed. Install with: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`pip install pyarrow`. Subsequent calls to cloud operations \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwill be ignored.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Uploading, downloading, and deleting from cloud storage requires pyarrow to be installed. Install with: `pip install pyarrow`. Subsequent calls to cloud operations will be ignored."
     ]
    }
   ],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .rl_module(_enable_rl_module_api=False)\n",
    "    .environment(env=\"random_maze_action_mask\")\\\n",
    "    .rollouts(num_rollout_workers=4, num_envs_per_worker=1)\\\n",
    "    .training(_enable_learner_api=False, train_batch_size=tune.grid_search([12000, 18000]), gamma=0.995, model=model, lr=0.0003,\\\n",
    "              clip_param=0.15, num_sgd_iter=tune.grid_search([3,5,7]), sgd_minibatch_size=tune.grid_search([2000, 4000]), entropy_coeff=0.0001,)\\\n",
    "    .environment(disable_env_checking=True)\\\n",
    "    .framework('torch')\\\n",
    "    )\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    tune_config=tune.TuneConfig(num_samples=3),\n",
    "    param_space=config.to_dict(),\n",
    "    run_config=air.RunConfig(stop={\"timesteps_total\": 1.5e6}, storage_path=\"./results\", name=\"ppo_hyperparam_search\"))\n",
    "\n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocktake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
