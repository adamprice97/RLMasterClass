{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Environments.random_maze import maze_game\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Dict\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.models import MODEL_DEFAULTS\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import copy\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.utils.torch_utils import FLOAT_MIN\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray import air, tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced RL Algorithms and RLLib!** \n",
    "\n",
    "The goal of this second notebook is to show you how I go about solving RL problems, using RLLib and more advanced algorithms. Hopefully it will give you an insight into types of problems that are encountered when working in RL and how they are solved. \n",
    "\n",
    "As a bit of a showcase, we are going to make the maze game for earlier harder, and discuss how to solve it. \n",
    "\n",
    "As in the previous notebook, we will start by looking at problem. You will notice that once again it is a maze game; however, this one will be a fair bit harder to solve. Run the cell below a few times, what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = maze_game()\n",
    "example.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the maze game is random! We will need a high level of generalisation to solve this problem! (the action space is the same)\n",
    "\n",
    "You will also notice that the state is split up by a dictionary. The base form (before wrapping) should present as much data as possible, we can always use wrappers later to reduce the data. \n",
    "\n",
    "Like all good ML, we will start with a baseline. We will produce a wrapper that stacks the into 3 planes that will make up the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stacked_maze(maze_game):\n",
    "    #overwrite the init to change the obs space\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #This wrapper updates the observation space. It is now best discribed a multibinary.\n",
    "        self.observation_space = spaces.MultiBinary((12,12,2))\n",
    "    \n",
    "    #overwrite create_state to change how the env handles the state\n",
    "    def create_state(self): #In this version, the goal never changes, so ignore it\n",
    "        state = np.stack([np.array(self.maze==self.wall, dtype=np.int8),\n",
    "                          np.array(self.maze==self.agent, dtype=np.int8)], axis=-1)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can introduce RLLib, we start by registiering the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to registier the env we want to use with RLLib before we can use it:\n",
    "def env_creator(env_config):\n",
    "    return stacked_maze()\n",
    "register_env(\"stacked_maze\", env_creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the model that we want to use. More on this config can be found here: https://github.com/ray-project/ray/blob/469f4d296a112f2ade556ea586a0f05811b34d32/rllib/models/catalog.py#L52\n",
    "\n",
    "We are using some convolution layers to reduce the dimensionality of the matrix inputs. These will get flattened and passed it the dense layers given in 'fcnet_hiddens'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.copy(MODEL_DEFAULTS)\n",
    "model.update({'fcnet_hiddens': [256, 128], 'fcnet_activation': 'relu', 'conv_filters': [[4, [3, 3], 1], [8, [3, 3], 1], [12, [3, 3], 1], [16, [3, 3], 1]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a config that will produce a trainer for learning on the \"stacked_maze\" environment. We wont worry too much about the learning params rn. At this stage I am mainly checking that my MDP is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .rl_module(_enable_rl_module_api=False)\n",
    "    .environment(env=\"stacked_maze\")\\\n",
    "    .rollouts(num_rollout_workers=4, num_envs_per_worker=1)\\\n",
    "    .training(_enable_learner_api=False, train_batch_size=2000, gamma=0.995, model=model, lr=0.001,  )\\\n",
    "    .environment(disable_env_checking=True)\\\n",
    "    .framework('torch')\\\n",
    ")\n",
    "trainer = config.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the trainer is built, we can start training! We are simply powering this with a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "\n",
    "for i in range(50):\n",
    "    print_results(trainer.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this didn't get a very good result, but it seems to have learnt something! So, it is now time to consider what hyperparameters we are using. In RL, we typically have 3-8 different hyperparameters to balance, which makes running hyperparameter searches very time-consuming. Luckily, we can generally reduce the range of our hyperparameter searches by using a bit of common sense. Thinking about our target problem, each episode the maze changes, so the agent needs to learn the general representation of the maze for the best result. \n",
    "\n",
    "Like, in supervised learning, for generalisation to form, we need to ensure that the DNN cannot overfit its data (overfitting is typically caused by parameters that give aggressive learning, like a high learning rate, or by a lack of data points to generalise across). To reduce overfitting in PPO, we change the following parameters:\n",
    "\n",
    "- train_batch_size: Increasing this makes PPO use more data for each policy update - letting the policy apply its update with more data can help with generalisation.\n",
    "- clip_param: PPO clips its policy less to prevent overly large updates. Reducing this value (default is .2) reduces the magnitude of the policy updates.\n",
    "- num_sgd_iter: PPO uses importance sampling to let it apply multiple updates with one training batch. The fewer iterations of the sample batch we do, the less we will fit the data. This can be set far higher (20-30) in very static problems. \n",
    "- sgd_minibatch_size: The size of the mini-batches that make up each update. Has similar effects as train_batch_size. By making it so large, we take the average across multiple episodes for each update.\n",
    "- entropy_coeff: Adding in a small amount of entropy loss to aid in exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .rl_module(_enable_rl_module_api=False)\n",
    "    .environment(env=\"stacked_maze\")\\\n",
    "    .rollouts(num_rollout_workers=4, num_envs_per_worker=1)\\\n",
    "    .training(_enable_learner_api=False, train_batch_size=15000, gamma=0.995, model=model, lr=0.0003,\\\n",
    "              clip_param=0.15, num_sgd_iter=4, sgd_minibatch_size=3000, entropy_coeff=0.0001,)\\\n",
    "    .environment(disable_env_checking=True)\\\n",
    "    .framework('torch')\\\n",
    ")\n",
    "trainer = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    print_results(trainer.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you saw better results with these more optimised hyperparameters. But, it seems that we still haven't solved the random maze game. Longer training would probably improve things, but we can also aid learning through the same action masking technique we looked at in the last notebook!\n",
    "\n",
    "To implement action masking with RLLib we again start by wrapping the target environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_maze_action_mask(maze_game):\n",
    "    #overwrite the init to change the obs space\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #This wrapper updates the observation space. It is now best discribed a multibinary.\n",
    "        self.observation_space = spaces.Dict({'observations': spaces.MultiBinary((12,12,2)), \n",
    "                                              'action_mask': spaces.MultiBinary(4)})\n",
    "    \n",
    "    #overwrite create_state to change how the env handles the state\n",
    "    def create_state(self):\n",
    "        state = {}\n",
    "        state['observations'] = np.stack([np.array(self.maze==self.wall, dtype=np.int8),\n",
    "                                  np.array(self.maze==self.agent, dtype=np.int8)], axis=-1)\n",
    "        state['action_mask'] = self.get_action_mask()\n",
    "        return state\n",
    "    \n",
    "    def get_action_mask(self):\n",
    "        action_mask = np.zeros(4,dtype=np.int8)\n",
    "        if self.y != 0:\n",
    "            if self.maze[self.x, self.y-1] != self.wall:\n",
    "                action_mask[0] = 1\n",
    "        if self.x != self.maze.shape[0]-1:\n",
    "            if self.maze[self.x+1, self.y] != self.wall:\n",
    "                action_mask[1] = 1\n",
    "        if self.y != self.maze.shape[1]-1:\n",
    "            if self.maze[self.x, self.y+1] != self.wall:\n",
    "                action_mask[2] = 1\n",
    "        if self.x != 0:\n",
    "            if self.maze[self.x-1, self.y] != self.wall:\n",
    "                action_mask[3] = 1\n",
    "        return action_mask\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return random_maze_action_mask()\n",
    "register_env(\"random_maze_action_mask\", env_creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will update the model that our policy will use. This one will look at the action_mask present in the state. It uses it to reduce the relevant logits to 'FLOAT_MIN'. When the policies action selector (a separate class that turns DNN logits into actions) gets these logits, the softmax will not select the masked-out logits as actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch, nn = try_import_torch()\n",
    "\n",
    "class ActionMaskModel(TorchModelV2, nn.Module):\n",
    "    \"\"\"PyTorch version of above ActionMaskingModel.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        model_config,\n",
    "        name,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        orig_space = getattr(obs_space, \"original_space\", obs_space)\n",
    "        assert (\n",
    "            isinstance(orig_space, Dict)\n",
    "            and \"action_mask\" in orig_space.spaces\n",
    "            and \"observations\" in orig_space.spaces\n",
    "        )\n",
    "\n",
    "        TorchModelV2.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name, **kwargs\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.internal_model = TorchFC(\n",
    "            orig_space[\"observations\"],\n",
    "            action_space,\n",
    "            num_outputs,\n",
    "            model_config,\n",
    "            name + \"_internal\",\n",
    "        )\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Extract the available actions tensor from the observation.\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "        # Compute the unmasked logits.\n",
    "        logits, _ = self.internal_model({\"obs\": input_dict[\"obs\"][\"observations\"]})\n",
    "        # Convert action_mask into a [0.0 || -inf]-type mask.\n",
    "        inf_mask = torch.clamp(torch.log(action_mask), min=FLOAT_MIN)\n",
    "        masked_logits = logits + inf_mask\n",
    "        # Return masked logits.\n",
    "        return masked_logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.internal_model.value_function()\n",
    "\n",
    "#Like an envionrment, we need to register our custom model.\n",
    "ModelCatalog.register_custom_model(\"RandomMazeModel\", ActionMaskModel)\n",
    "model = copy.copy(MODEL_DEFAULTS)\n",
    "model.update({'custom_model': \"RandomMazeModel\", 'custom_model_config': {},})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train again using action masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .rl_module(_enable_rl_module_api=False)\n",
    "    .environment(env=\"random_maze_action_mask\")\\\n",
    "    .rollouts(num_rollout_workers=4, num_envs_per_worker=1)\\\n",
    "    .training(_enable_learner_api=False, train_batch_size=15000, gamma=0.995, model=model, lr=0.0003,\\\n",
    "              clip_param=0.15, num_sgd_iter=4, sgd_minibatch_size=3000, entropy_coeff=0.0001,)\\\n",
    "    .environment(disable_env_checking=True)\\\n",
    "    .framework('torch')\\\n",
    ")\n",
    "trainer = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    print_results(trainer.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see that, unsurprisingly, learning has gone a lot better with action masking. PPO still isn't solving the problem, though. Now, this is most likely because we haven't let training go on for long enough, but this is also a good opportunity to look at parameter tuning. \n",
    "\n",
    "Running a hyperparameter search can be time-consuming, so I have already run one and kept the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-10-25 17:32:23</td></tr>\n",
       "<tr><td>Running for: </td><td>02:04:56.28        </td></tr>\n",
       "<tr><td>Memory:      </td><td>30.3/79.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 13.0/20 CPUs, 1.0/1 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                             </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  num_sgd_iter</th><th style=\"text-align: right;\">  sgd_minibatch_size</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00000</td><td>TERMINATED</td><td>127.0.0.1:11688</td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         279.398</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">   2.247</td><td style=\"text-align: right;\">                48.1</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            263.1 </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00001</td><td>TERMINATED</td><td>127.0.0.1:6416 </td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         295.171</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">   4.096</td><td style=\"text-align: right;\">                47.5</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            269.66</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00002</td><td>TERMINATED</td><td>127.0.0.1:1908 </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         271.755</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">   5.333</td><td style=\"text-align: right;\">                46.7</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            272.32</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00003</td><td>TERMINATED</td><td>127.0.0.1:20528</td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         296.525</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">   2.704</td><td style=\"text-align: right;\">                47.5</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            273.56</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00004</td><td>TERMINATED</td><td>127.0.0.1:26372</td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         309.386</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">   8.224</td><td style=\"text-align: right;\">                47.9</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            238.4 </td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00005</td><td>TERMINATED</td><td>127.0.0.1:20272</td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         279.82 </td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">  11.331</td><td style=\"text-align: right;\">                47.1</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            232.38</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00006</td><td>TERMINATED</td><td>127.0.0.1:14960</td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         319.457</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">   5.894</td><td style=\"text-align: right;\">                47.9</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            241.66</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00007</td><td>TERMINATED</td><td>127.0.0.1:26696</td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         336.087</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">  -1.614</td><td style=\"text-align: right;\">                48.1</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            286.68</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00008</td><td>TERMINATED</td><td>127.0.0.1:27040</td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         296.075</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">   4.711</td><td style=\"text-align: right;\">                47.3</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            268.52</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00009</td><td>TERMINATED</td><td>127.0.0.1:632  </td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         283.684</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">   9.698</td><td style=\"text-align: right;\">                47.5</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            233.68</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00010</td><td>TERMINATED</td><td>127.0.0.1:2572 </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         266.891</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">   5.235</td><td style=\"text-align: right;\">                47.5</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            263.28</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00011</td><td>TERMINATED</td><td>127.0.0.1:26656</td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         284.425</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">   1.836</td><td style=\"text-align: right;\">                47.3</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            272.22</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00012</td><td>TERMINATED</td><td>127.0.0.1:1288 </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         271.698</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">   1.113</td><td style=\"text-align: right;\">                47.9</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            274.44</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00013</td><td>TERMINATED</td><td>127.0.0.1:24928</td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         297.129</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">   8.382</td><td style=\"text-align: right;\">                47.5</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            246.84</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00014</td><td>TERMINATED</td><td>127.0.0.1:9504 </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         264.584</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">  -0.288</td><td style=\"text-align: right;\">                47.5</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            283.44</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00015</td><td>TERMINATED</td><td>127.0.0.1:4200 </td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         268.935</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">   2.453</td><td style=\"text-align: right;\">                46.5</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            271.06</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00016</td><td>TERMINATED</td><td>127.0.0.1:9832 </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         273.873</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">  11.29 </td><td style=\"text-align: right;\">                47.9</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            227.78</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00017</td><td>TERMINATED</td><td>127.0.0.1:18720</td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         284.573</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">  10.392</td><td style=\"text-align: right;\">                47.3</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            236.76</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00018</td><td>TERMINATED</td><td>127.0.0.1:15120</td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         269.198</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">   9.922</td><td style=\"text-align: right;\">                47.5</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            241.46</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00019</td><td>TERMINATED</td><td>127.0.0.1:19876</td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             12000</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         289.267</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">  -2.957</td><td style=\"text-align: right;\">                47.7</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            285.08</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00020</td><td>TERMINATED</td><td>127.0.0.1:9936 </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         287.229</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">  10.954</td><td style=\"text-align: right;\">                47.3</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            231.14</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00021</td><td>TERMINATED</td><td>127.0.0.1:2612 </td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                2000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         307.475</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">   8.904</td><td style=\"text-align: right;\">                47.9</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            241.62</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00022</td><td>TERMINATED</td><td>127.0.0.1:1236 </td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         282.388</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">   9.461</td><td style=\"text-align: right;\">                47.5</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            241.06</td></tr>\n",
       "<tr><td>PPO_random_maze_action_mask_9f288_00023</td><td>TERMINATED</td><td>127.0.0.1:18044</td><td style=\"text-align: right;\">             5</td><td style=\"text-align: right;\">                4000</td><td style=\"text-align: right;\">             18000</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         283.63 </td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">   8.721</td><td style=\"text-align: right;\">                47.7</td><td style=\"text-align: right;\">                 -50</td><td style=\"text-align: right;\">            238.44</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 15:27:27,049\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,050\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,053\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,054\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,057\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,058\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,061\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,062\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,065\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,066\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,069\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,070\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,073\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,073\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,077\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,077\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,088\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,088\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,091\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,092\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,095\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,096\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,099\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,099\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,102\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,103\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,106\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,106\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,109\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,110\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,113\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,114\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,125\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,125\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,128\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,129\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,132\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,133\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,136\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,136\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,139\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,140\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "2023-10-25 15:27:27,143\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:27,143\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(pid=11688)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23808)\u001b[0m   File \"c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23808)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23808)\u001b[0m   File \"c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23808)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23808)\u001b[0m   File \"c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 470, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23808)\u001b[0m     self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23808)\u001b[0m   File \"c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm_config.py\", line 2956, in get_multi_agent_setup\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23808)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23808)\u001b[0m ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:34,404\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:34,404\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:34,404\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=23804)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=1224)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23804)\u001b[0m 2023-10-25 15:27:45,000\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m 2023-10-25 15:27:45,439\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m 2023-10-25 15:27:45,439\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m 2023-10-25 15:27:45,446\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m 2023-10-25 15:27:45,446\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m 2023-10-25 15:27:45,446\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m 2023-10-25 15:27:45,446\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m 2023-10-25 15:27:45,446\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m 2023-10-25 15:27:45,446\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m 2023-10-25 15:27:45,446\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m 2023-10-25 15:27:45,451\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "2023-10-25 15:27:48,639\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:27:48,640\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m Trainable.setup took 14.215 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:50,641\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=22636)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:45,509\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:45,511\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:45,511\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:45,608\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:45,608\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:45,608\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:45,608\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:45,608\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:45,609\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:45,609\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11688)\u001b[0m 2023-10-25 15:27:48,585\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10296)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "2023-10-25 15:32:29,671\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=6416)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:40,069\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:40,069\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:40,070\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=19492)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-10-25 15:32:50,263\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-10-25 15:32:50,266\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-10-25 15:32:50,267\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-10-25 15:32:50,273\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-10-25 15:32:50,273\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-10-25 15:32:50,273\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-10-25 15:32:50,274\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-10-25 15:32:50,274\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-10-25 15:32:50,274\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-10-25 15:32:50,274\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m 2023-10-25 15:32:50,278\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "2023-10-25 15:32:52,653\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-25 15:32:52,653\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m Trainable.setup took 12.563 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19492)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:54,580\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 15:37:32,174\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=1908)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:50,887\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:50,890\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:50,890\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:50,990\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:50,990\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:50,990\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:50,990\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:50,990\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:50,990\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:50,990\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=6416)\u001b[0m 2023-10-25 15:32:52,616\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:37:59,381\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:37:59,381\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=11996)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:37:59,381\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=21168)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m 2023-10-25 15:38:09,815\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m 2023-10-25 15:38:09,818\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m 2023-10-25 15:38:09,819\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m 2023-10-25 15:38:09,825\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m 2023-10-25 15:38:09,825\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m 2023-10-25 15:38:09,825\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m 2023-10-25 15:38:09,825\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m 2023-10-25 15:38:09,825\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m 2023-10-25 15:38:09,826\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m 2023-10-25 15:38:09,826\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m 2023-10-25 15:38:09,830\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m Trainable.setup took 12.708 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:14,047\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11996)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "2023-10-25 15:42:34,544\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=20528)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:10,327\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:10,329\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:10,329\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:10,427\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:10,427\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:10,427\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:10,427\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:10,427\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:10,427\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:10,427\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1908)\u001b[0m 2023-10-25 15:38:12,073\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:42:55,418\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:42:55,418\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=24200)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:42:55,418\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=12980)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m 2023-10-25 15:43:05,714\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m 2023-10-25 15:43:05,718\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m 2023-10-25 15:43:05,718\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m 2023-10-25 15:43:05,724\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m 2023-10-25 15:43:05,724\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m 2023-10-25 15:43:05,724\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m 2023-10-25 15:43:05,724\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m 2023-10-25 15:43:05,725\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m 2023-10-25 15:43:05,725\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m 2023-10-25 15:43:05,725\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m 2023-10-25 15:43:05,729\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m Trainable.setup took 12.723 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:10,114\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24200)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "2023-10-25 15:47:36,999\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=26372)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:06,375\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:06,377\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:06,378\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:06,462\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:06,462\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:06,462\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:06,463\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:06,463\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:06,463\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:06,463\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20528)\u001b[0m 2023-10-25 15:43:08,125\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:16,242\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:16,243\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=20748)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:16,242\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=17612)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20748)\u001b[0m 2023-10-25 15:48:26,797\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m 2023-10-25 15:48:27,325\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m 2023-10-25 15:48:27,325\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m 2023-10-25 15:48:27,330\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m 2023-10-25 15:48:27,330\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m 2023-10-25 15:48:27,330\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m 2023-10-25 15:48:27,330\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m 2023-10-25 15:48:27,331\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m 2023-10-25 15:48:27,331\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m 2023-10-25 15:48:27,331\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m 2023-10-25 15:48:27,334\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m Trainable.setup took 12.969 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15220)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=24020)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:27,401\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:27,403\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:27,403\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:27,491\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:27,491\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:27,491\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:27,492\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:27,492\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:27,492\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:27,492\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:29,196\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26372)\u001b[0m 2023-10-25 15:48:32,442\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 15:52:39,627\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=20272)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:53:49,217\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:53:49,217\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:53:49,217\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=9896)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3700)\u001b[0m 2023-10-25 15:53:59,703\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m 2023-10-25 15:53:59,758\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m 2023-10-25 15:53:59,758\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m 2023-10-25 15:53:59,764\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m 2023-10-25 15:53:59,764\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m 2023-10-25 15:53:59,765\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m 2023-10-25 15:53:59,765\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m 2023-10-25 15:53:59,765\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m 2023-10-25 15:53:59,765\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m 2023-10-25 15:53:59,765\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m 2023-10-25 15:53:59,770\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m Trainable.setup took 12.896 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9896)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=24672)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24672)\u001b[0m 2023-10-25 15:54:00,202\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:54:00,258\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:54:00,258\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:54:00,339\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:54:00,339\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:54:00,339\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:54:00,340\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:54:00,340\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:54:00,340\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:54:00,340\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:54:02,095\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=20272)\u001b[0m 2023-10-25 15:54:05,357\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 15:57:42,110\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=14960)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:58:52,270\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:58:52,270\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:58:52,271\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=4880)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4880)\u001b[0m 2023-10-25 15:59:03,143\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m 2023-10-25 15:59:03,595\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m 2023-10-25 15:59:03,595\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m 2023-10-25 15:59:03,600\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m 2023-10-25 15:59:03,601\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m 2023-10-25 15:59:03,601\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m 2023-10-25 15:59:03,601\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m 2023-10-25 15:59:03,601\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m 2023-10-25 15:59:03,602\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m 2023-10-25 15:59:03,602\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m 2023-10-25 15:59:03,608\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m Trainable.setup took 13.293 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24268)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=2752)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:03,652\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:03,654\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:03,654\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:03,746\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:03,746\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:03,746\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:03,746\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:03,747\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:03,747\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:03,747\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:05,546\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=14960)\u001b[0m 2023-10-25 15:59:08,712\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 16:02:44,413\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=26696)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:38,298\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:38,298\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:38,298\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=10364)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10364)\u001b[0m 2023-10-25 16:04:52,582\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m 2023-10-25 16:04:52,916\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m 2023-10-25 16:04:52,916\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m 2023-10-25 16:04:52,924\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m 2023-10-25 16:04:52,924\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m 2023-10-25 16:04:52,924\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m 2023-10-25 16:04:52,925\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m 2023-10-25 16:04:52,925\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m 2023-10-25 16:04:52,925\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m 2023-10-25 16:04:52,925\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m 2023-10-25 16:04:52,931\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m Trainable.setup took 18.256 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26828)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=22556)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:53,000\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:53,002\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:53,002\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:53,124\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:53,124\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:53,124\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:53,125\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:53,125\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:53,125\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:53,125\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:04:56,507\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26696)\u001b[0m 2023-10-25 16:05:00,881\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 16:07:46,650\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=27040)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:45,050\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:45,050\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:45,051\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=4800)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4800)\u001b[0m 2023-10-25 16:10:58,930\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m 2023-10-25 16:10:59,015\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m 2023-10-25 16:10:59,016\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m 2023-10-25 16:10:59,023\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m 2023-10-25 16:10:59,023\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m 2023-10-25 16:10:59,023\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m 2023-10-25 16:10:59,024\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m 2023-10-25 16:10:59,024\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m 2023-10-25 16:10:59,024\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m 2023-10-25 16:10:59,024\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m 2023-10-25 16:10:59,028\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m Trainable.setup took 17.429 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24428)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=6380)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:59,111\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:59,113\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:59,113\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:59,241\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:59,241\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:59,241\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:59,241\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:59,241\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:59,242\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:10:59,242\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:11:02,436\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=27040)\u001b[0m 2023-10-25 16:11:05,064\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 16:12:49,114\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=632)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:09,963\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:09,963\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:09,964\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=11340)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11340)\u001b[0m 2023-10-25 16:16:19,955\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m 2023-10-25 16:16:20,747\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m 2023-10-25 16:16:20,747\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m 2023-10-25 16:16:20,752\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m 2023-10-25 16:16:20,752\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m 2023-10-25 16:16:20,752\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m 2023-10-25 16:16:20,752\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m 2023-10-25 16:16:20,752\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m 2023-10-25 16:16:20,753\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m 2023-10-25 16:16:20,753\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m 2023-10-25 16:16:20,755\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m Trainable.setup took 12.639 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11016)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=10328)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:24,535\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 16:17:51,099\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:20,802\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:20,804\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:20,804\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:20,901\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:20,901\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:20,901\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:20,901\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:20,901\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:20,902\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:20,902\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=632)\u001b[0m 2023-10-25 16:16:22,587\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=2572)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:18,123\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:18,123\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=11592)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:18,123\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5748)\u001b[0m 2023-10-25 16:21:28,435\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m 2023-10-25 16:21:28,883\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m 2023-10-25 16:21:28,883\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m 2023-10-25 16:21:28,890\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m 2023-10-25 16:21:28,890\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m 2023-10-25 16:21:28,890\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m 2023-10-25 16:21:28,891\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m 2023-10-25 16:21:28,891\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m 2023-10-25 16:21:28,891\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m 2023-10-25 16:21:28,891\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m 2023-10-25 16:21:28,895\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m Trainable.setup took 12.672 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2360)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:32,719\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "2023-10-25 16:22:53,348\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:28,976\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:28,978\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:28,978\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:29,078\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:29,079\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:29,079\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:29,079\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:29,079\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:29,079\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:29,079\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2572)\u001b[0m 2023-10-25 16:21:30,777\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=26656)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:09,130\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:09,130\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=17652)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:09,111\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m 2023-10-25 16:26:19,433\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m 2023-10-25 16:26:19,436\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m 2023-10-25 16:26:19,437\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m 2023-10-25 16:26:19,442\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m 2023-10-25 16:26:19,442\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m 2023-10-25 16:26:19,443\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m 2023-10-25 16:26:19,443\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m 2023-10-25 16:26:19,443\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m 2023-10-25 16:26:19,443\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m 2023-10-25 16:26:19,443\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m 2023-10-25 16:26:19,448\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m Trainable.setup took 12.892 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17652)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=20312)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:23,965\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 16:27:46,405\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.372 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:27:46,407\tWARNING util.py:315 -- The `process_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:27:46,407\tWARNING util.py:315 -- Processing trial results took 1.375 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-10-25 16:27:46,409\tWARNING util.py:315 -- The `process_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:27:55,778\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.253 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:27:55,780\tWARNING util.py:315 -- The `process_trial_result` operation took 1.255 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:27:55,780\tWARNING util.py:315 -- Processing trial results took 1.255 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-10-25 16:27:55,781\tWARNING util.py:315 -- The `process_trial_result` operation took 1.256 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:27:56,538\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "2023-10-25 16:28:26,740\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.400 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:28:26,740\tWARNING util.py:315 -- The `process_trial_result` operation took 1.401 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:28:26,741\tWARNING util.py:315 -- Processing trial results took 1.402 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-10-25 16:28:26,742\tWARNING util.py:315 -- The `process_trial_result` operation took 1.403 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:28:37,294\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.112 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:28:37,296\tWARNING util.py:315 -- The `process_trial_result` operation took 1.114 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:28:37,296\tWARNING util.py:315 -- Processing trial results took 1.114 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-10-25 16:28:37,297\tWARNING util.py:315 -- The `process_trial_result` operation took 1.115 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:28:42,948\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.037 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:28:42,950\tWARNING util.py:315 -- The `process_trial_result` operation took 1.040 s, which may be a performance bottleneck.\n",
      "2023-10-25 16:28:42,951\tWARNING util.py:315 -- Processing trial results took 1.041 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-10-25 16:28:42,951\tWARNING util.py:315 -- The `process_trial_result` operation took 1.041 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20312)\u001b[0m 2023-10-25 16:26:20,144\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:20,220\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:20,220\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:20,308\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:20,308\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:20,308\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:20,308\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:20,308\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:20,308\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:20,308\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=26656)\u001b[0m 2023-10-25 16:26:22,006\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=1288)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:24,709\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:24,710\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=23360)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:24,709\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24400)\u001b[0m 2023-10-25 16:31:34,877\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m 2023-10-25 16:31:35,552\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m 2023-10-25 16:31:35,553\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m 2023-10-25 16:31:35,557\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m 2023-10-25 16:31:35,557\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m 2023-10-25 16:31:35,557\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m 2023-10-25 16:31:35,557\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m 2023-10-25 16:31:35,557\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m 2023-10-25 16:31:35,557\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m 2023-10-25 16:31:35,557\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m 2023-10-25 16:31:35,561\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m Trainable.setup took 12.729 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25100)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=8784)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:35,611\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:40,290\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 16:32:59,624\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:35,613\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:35,613\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:35,716\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:35,716\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:35,716\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:35,717\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:35,717\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:35,717\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:35,717\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1288)\u001b[0m 2023-10-25 16:31:37,422\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=24928)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:19,670\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:19,671\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:19,671\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=26736)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m 2023-10-25 16:36:29,711\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m 2023-10-25 16:36:29,715\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m 2023-10-25 16:36:29,715\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m 2023-10-25 16:36:29,721\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m 2023-10-25 16:36:29,721\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m 2023-10-25 16:36:29,721\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m 2023-10-25 16:36:29,722\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m 2023-10-25 16:36:29,722\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m 2023-10-25 16:36:29,722\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m 2023-10-25 16:36:29,722\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m 2023-10-25 16:36:29,725\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m Trainable.setup took 12.541 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:35,135\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=4132)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4840)\u001b[0m 2023-10-25 16:36:30,341\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:30,409\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:30,410\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:30,503\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:30,503\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:30,503\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:30,503\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:30,504\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:30,504\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:30,504\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=24928)\u001b[0m 2023-10-25 16:36:32,196\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26736)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "2023-10-25 16:38:02,053\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=9504)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:40,312\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:40,312\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:40,312\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=10336)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10336)\u001b[0m 2023-10-25 16:41:50,845\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m 2023-10-25 16:41:51,498\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m 2023-10-25 16:41:51,498\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m 2023-10-25 16:41:51,505\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m 2023-10-25 16:41:51,505\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m 2023-10-25 16:41:51,505\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m 2023-10-25 16:41:51,506\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m 2023-10-25 16:41:51,506\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m 2023-10-25 16:41:51,506\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m 2023-10-25 16:41:51,506\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m 2023-10-25 16:41:51,511\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m Trainable.setup took 13.157 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26332)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=2120)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2120)\u001b[0m 2023-10-25 16:41:51,565\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:56,303\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 16:43:04,359\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:51,643\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:51,643\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:51,731\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:51,732\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:51,732\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:51,732\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:51,732\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:51,732\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:51,732\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9504)\u001b[0m 2023-10-25 16:41:53,454\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=4200)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:28,214\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:28,214\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:28,214\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=14252)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m 2023-10-25 16:46:38,321\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m 2023-10-25 16:46:38,324\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m 2023-10-25 16:46:38,324\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m 2023-10-25 16:46:38,329\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m 2023-10-25 16:46:38,329\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m 2023-10-25 16:46:38,329\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m 2023-10-25 16:46:38,329\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m 2023-10-25 16:46:38,329\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m 2023-10-25 16:46:38,329\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m 2023-10-25 16:46:38,329\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m 2023-10-25 16:46:38,332\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m Trainable.setup took 12.846 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14252)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=17252)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:39,211\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:39,214\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:39,214\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:39,316\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:39,316\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:39,316\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:39,317\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:39,317\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:39,317\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:39,317\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:41,044\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=4200)\u001b[0m 2023-10-25 16:46:44,055\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 16:48:06,533\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=9832)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:20,123\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:20,123\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:20,124\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=2684)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m 2023-10-25 16:51:30,404\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m 2023-10-25 16:51:30,407\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m 2023-10-25 16:51:30,407\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m 2023-10-25 16:51:30,413\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m 2023-10-25 16:51:30,413\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m 2023-10-25 16:51:30,414\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m 2023-10-25 16:51:30,414\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m 2023-10-25 16:51:30,414\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m 2023-10-25 16:51:30,414\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m 2023-10-25 16:51:30,414\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m 2023-10-25 16:51:30,418\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m Trainable.setup took 12.813 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:34,850\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=2900)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2684)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "2023-10-25 16:53:09,104\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:31,053\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:31,056\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:31,056\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:31,169\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:31,169\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:31,169\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:31,169\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:31,169\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:31,169\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:31,169\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9832)\u001b[0m 2023-10-25 16:51:32,922\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=18720)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:17,979\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:17,980\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=4880)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:17,979\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4880)\u001b[0m 2023-10-25 16:56:28,206\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m 2023-10-25 16:56:28,391\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m 2023-10-25 16:56:28,391\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m 2023-10-25 16:56:28,399\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m 2023-10-25 16:56:28,399\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m 2023-10-25 16:56:28,399\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m 2023-10-25 16:56:28,399\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m 2023-10-25 16:56:28,400\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m 2023-10-25 16:56:28,400\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m 2023-10-25 16:56:28,400\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m 2023-10-25 16:56:28,405\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m Trainable.setup took 12.813 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11824)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:32,715\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=26148)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "2023-10-25 16:58:11,558\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:28,905\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:28,908\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:28,908\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:29,011\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:29,011\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:29,011\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:29,011\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:29,011\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:29,011\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:29,011\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18720)\u001b[0m 2023-10-25 16:56:30,775\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=15120)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:26,330\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:26,330\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=18628)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:26,330\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18628)\u001b[0m 2023-10-25 17:01:36,432\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:37,330\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:37,330\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m 2023-10-25 17:01:37,285\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m 2023-10-25 17:01:37,285\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m 2023-10-25 17:01:37,285\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m 2023-10-25 17:01:37,285\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m 2023-10-25 17:01:37,285\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m 2023-10-25 17:01:37,285\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m 2023-10-25 17:01:37,285\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m 2023-10-25 17:01:37,289\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m Trainable.setup took 12.852 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:41,099\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=21016)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "2023-10-25 17:03:13,950\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m 2023-10-25 17:01:37,277\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m 2023-10-25 17:01:37,280\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21016)\u001b[0m 2023-10-25 17:01:37,281\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:37,424\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:37,424\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:37,424\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:37,424\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:37,424\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:37,424\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:37,424\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=15120)\u001b[0m 2023-10-25 17:01:39,166\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=19876)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:20,189\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:20,190\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=23116)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:20,189\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23116)\u001b[0m 2023-10-25 17:06:30,307\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:31,136\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:31,136\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m 2023-10-25 17:06:31,091\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m 2023-10-25 17:06:31,091\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m 2023-10-25 17:06:31,091\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m 2023-10-25 17:06:31,091\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m 2023-10-25 17:06:31,091\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m 2023-10-25 17:06:31,091\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m 2023-10-25 17:06:31,091\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m 2023-10-25 17:06:31,094\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m Trainable.setup took 12.831 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=15620)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:34,955\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 17:08:16,173\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15620)\u001b[0m 2023-10-25 17:06:31,082\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m 2023-10-25 17:06:31,086\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26024)\u001b[0m 2023-10-25 17:06:31,086\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:31,230\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:31,230\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:31,230\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:31,230\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:31,231\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:31,231\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:31,231\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19876)\u001b[0m 2023-10-25 17:06:33,004\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=9936)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:34,149\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:34,149\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=22964)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:34,148\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22964)\u001b[0m 2023-10-25 17:11:46,103\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m 2023-10-25 17:11:46,726\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m 2023-10-25 17:11:46,726\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m 2023-10-25 17:11:46,733\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m 2023-10-25 17:11:46,734\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m 2023-10-25 17:11:46,734\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m 2023-10-25 17:11:46,734\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m 2023-10-25 17:11:46,734\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m 2023-10-25 17:11:46,735\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m 2023-10-25 17:11:46,735\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m 2023-10-25 17:11:46,740\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m Trainable.setup took 14.614 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8704)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=8488)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:46,811\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:46,814\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:46,814\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:46,912\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:46,912\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:46,913\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:46,913\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:46,913\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:46,913\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:46,913\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:48,745\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=9936)\u001b[0m 2023-10-25 17:11:51,993\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 17:13:18,628\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=2612)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:46,459\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:46,459\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:46,459\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=25904)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m 2023-10-25 17:16:56,505\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m 2023-10-25 17:16:56,508\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m 2023-10-25 17:16:56,509\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m 2023-10-25 17:16:56,516\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m 2023-10-25 17:16:56,517\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m 2023-10-25 17:16:56,517\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m 2023-10-25 17:16:56,517\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m 2023-10-25 17:16:56,517\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m 2023-10-25 17:16:56,517\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m 2023-10-25 17:16:56,517\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m 2023-10-25 17:16:56,522\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m Trainable.setup took 12.685 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25904)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=26104)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26104)\u001b[0m 2023-10-25 17:16:57,313\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:57,365\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:57,365\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:57,452\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:57,452\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:57,452\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:57,453\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:57,453\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:57,453\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:57,453\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:16:59,128\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2612)\u001b[0m 2023-10-25 17:17:02,077\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 17:18:21,025\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=1236)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:17,439\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:17,439\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:17,440\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=22056)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m 2023-10-25 17:22:28,672\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m 2023-10-25 17:22:28,676\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m 2023-10-25 17:22:28,676\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m 2023-10-25 17:22:28,683\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m 2023-10-25 17:22:28,684\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m 2023-10-25 17:22:28,684\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m 2023-10-25 17:22:28,684\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m 2023-10-25 17:22:28,684\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m 2023-10-25 17:22:28,684\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m 2023-10-25 17:22:28,684\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m 2023-10-25 17:22:28,690\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m Trainable.setup took 14.126 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:35,243\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27312)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:29,521\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:29,524\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:29,524\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:29,614\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:29,614\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:29,614\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:29,615\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:29,615\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:29,615\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:29,615\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=1236)\u001b[0m 2023-10-25 17:22:31,544\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22056)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "2023-10-25 17:23:23,594\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "\u001b[2m\u001b[36m(pid=18044)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18044)\u001b[0m 2023-10-25 17:27:24,093\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=18044)\u001b[0m 2023-10-25 17:27:24,093\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(PPO pid=18044)\u001b[0m 2023-10-25 17:27:24,094\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=6512)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6512)\u001b[0m 2023-10-25 17:27:35,022\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m 2023-10-25 17:27:35,707\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m 2023-10-25 17:27:35,708\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m 2023-10-25 17:27:35,715\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m 2023-10-25 17:27:35,715\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m 2023-10-25 17:27:35,715\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m 2023-10-25 17:27:35,716\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m 2023-10-25 17:27:35,716\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m 2023-10-25 17:27:35,716\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m 2023-10-25 17:27:35,716\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m 2023-10-25 17:27:35,720\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18044)\u001b[0m Trainable.setup took 13.499 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=18044)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2348)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=2348)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=18044)\u001b[0m 2023-10-25 17:27:35,758\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=18044)\u001b[0m 2023-10-25 17:27:40,539\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2023-10-25 17:28:26,257\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "2023-10-25 17:32:23,320\tWARNING tune.py:1122 -- Trial Runner checkpointing failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Adam/ray_results/ppo_hyperparam_search/PPO_a2c68_00000_0_2023-10-25_15-20-23', which is outside base dir 'C:\\Users\\Adam\\ray_results\\ppo_hyperparam_search'\n",
      "2023-10-25 17:32:23,916\tINFO tune.py:1148 -- Total run time: 7496.89 seconds (7496.26 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .rl_module(_enable_rl_module_api=False)\n",
    "    .environment(env=\"random_maze_action_mask\")\\\n",
    "    .rollouts(num_rollout_workers=12, num_envs_per_worker=1)\\\n",
    "    .training(_enable_learner_api=False, train_batch_size=tune.grid_search([12000, 18000]), gamma=0.995, model=model, lr=0.0003,\\\n",
    "              clip_param=0.15, num_sgd_iter=tune.grid_search([3,5]), sgd_minibatch_size=tune.grid_search([2000, 4000]), entropy_coeff=0.0001,)\\\n",
    "    .environment(disable_env_checking=True)\\\n",
    "    .framework('torch')\\\n",
    "    .resources(num_gpus=1)\n",
    "    )\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    tune_config=tune.TuneConfig(num_samples=1),\n",
    "    param_space=config.to_dict(),\n",
    "    run_config=air.RunConfig(stop={\"timesteps_total\": 1.5e6}, storage_path=\"./results\", name=\"ppo_hyperparam_search\"))\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take a look at these results, we will use tensorboard. Running the command below will launch tensorboard, but you will want to pull it out to the console if you want to use the notebook and tensorboard at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adam\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "'tensorboard' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir '/Users/<username>/RL Master Class/RLMasterClass/results/ppo_hyperparam_search'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, it might be wise to investigate the performance of a different algorithm, for example a DQN. I'm leavning this as an open task, feel free to explore DQNs or make a start on anything else. From here, I'd like to encourge you to work on anything that you might find interesting! \n",
    "\n",
    "Some ideas: \n",
    "\n",
    "- Apply DQN to this maze problem, and try to find an optimal learning set up. \n",
    "\n",
    "- Modify the Maze game in some way, and see how that affects learning. \n",
    "\n",
    "- Create your own MDP to try and further your understanding of how we make and solve MDPs. A good challenge is a two player game like tictactoe or connect4.\n",
    "\n",
    "- Find some other benchmarks to try and solve. Gynasium has many available: https://gymnasium.farama.org/index.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocktake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
